# 课时一 书生·浦语大模型全链路开源开放体系

[作业要求](https://aicarrier.feishu.cn/wiki/Vv4swUFMni5DiMkcasUczUp9nid#LSBkd2cTHorhsAx5jZAcO0B3nqe)

## 视频笔记

[视频链接](https://www.bilibili.com/video/BV1Vx421X72D/)



### 书生·浦语大模型介绍
- **全年度开源体系**：介绍了书生·浦语大模型的发展历程、特点，包括轻量级和重量级模型以及不同能力的模型。
- **通用大模型趋势**：强调了通用大模型成为人工智能发展趋势，以及书生·浦语大模型的开源历程。

### 模型性能与应用
- **20B模型性能**：展示了20B模型在推理、数学、代码等方面的性能，与GPT-3.5和Gemini Pro等模型进行了比较。
- **综合性能**：讨论了模型的综合性能达到同量级开源模型领先水平，并介绍了模型的计算能力和数据分析功能。

### 模型选型与应用流程
- **模型选型**：从模型选型到应用的整个流程，包括各个环节需要做的事情。
- **全链条工具体系**：介绍了书生·浦语的全链条工具体系和开源数据集，包括数据、预训练、微调、部署、评测、应用等环节。

### Open Compass 2.0评测体系
- **评测体系开发**：介绍了Open Compass 2.0思南大模型评测体系的开发和开源，以及评测基准社区的建立。
- **评测集适配**：强调了Open Compass已经适配超过100个评测集，是国内最完善的评测体系之一。

### 推理和部署工具
- **开源社区趋势**：讨论了英特尔开源模型推理和部署工具的评测和发展趋势。
- **智能体框架**：介绍了智能体框架Legend和多媒体多模态智能体工具箱的使用和开发。



## 技术报告笔记

[InternLM2 技术报告](https://arxiv.org/pdf/2403.17297.pdf)

[pdf](2403.17297.pdf)

### 总结：

- 模型介绍：
   - InternLM2是一个开源的大型语言模型，旨在通过创新的预训练和优化技术，在六个维度和30个基准测试中超越其前身。
   - 模型通过精心准备的多样化数据类型（包括文本、代码和长文本数据）进行预训练，有效捕捉长期依赖关系。

- 技术特点：
    - 采用Group Query Attention (GQA)技术，提高长序列推理的效率。
    - 通过Supervised Fine-Tuning (SFT)和Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF)策略进行模型对齐，解决人类偏好冲突和奖励黑客问题。

- 模型性能：
    - InternLM2在长文本建模方面表现出色，能够在200k的“针堆”测试中准确识别所有“针”。
    - 通过不同训练阶段和模型大小的发布，提供了模型演化的洞察。

- 数据准备：
    - 详细介绍了文本数据、代码数据和长文本数据的预处理流程。
    - 强调了数据过滤、去重和质量筛选的重要性。

- 训练框架：
    - 使用InternEvo框架进行模型训练，支持高效的GPU内存管理和通信优化。
    - 介绍了模型结构的设计，包括对Transformer架构的改进。

- 评估与分析：
    - 在多个下游任务上评估模型性能，包括语言和知识、推理和数学、编码等。
    - 对模型在对齐任务上的性能进行了评估，包括英语和中文的主观评估。

### 学习笔记：

- 长文本建模：InternLM2通过GQA和长文本预训练数据，提高了处理长文本的能力。
- 模型对齐：COOL RLHF策略通过条件奖励模型和多轮在线RLHF，提高了模型与人类偏好的一致性。
- 数据准备：在预训练过程中，对数据的筛选和处理至关重要，这直接影响模型的性能和安全性。
- 训练框架：InternEvo框架通过混合并行和冗余分片技术，提高了模型训练的效率和可扩展性。
- 评估方法：模型的评估不仅要关注客观性能指标，还要考虑主观评估，以确保模型的输出与人类偏好相符。